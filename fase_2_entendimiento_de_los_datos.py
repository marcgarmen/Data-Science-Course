# -*- coding: utf-8 -*-
"""Fase 2: Entendimiento de los datos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sHXybWAJlJzlmkmytmcPwLI7cJnEa1zY

Usando Pandas para cargar y explorar datos en Python
Ya con el archivo Excel cargado en Google Colab:

Crearemos una celda de código nueva para cada línea de código.
Corremos cada celda para detectar posibles errores antes de avanzar, además de ver los resultados.
Importamos la librería Pandas y la asignamos a la variable pd
"""

import pandas as pd

"""Creamos la variable df de tipo DataFrame (tabla) para cargar los datos del archivo Excel con la función read_excel( ) de la librería Pandas:


"""

df = pd.read_excel('Datos.xlsx')

"""Usamos la función head( )  para comprobar que los datos se cargaron correctemente en el DataFrame viendo los primeros 5 registros:


"""

df.head()

"""Aplicamos el método shape para conocer la forma del DataFrame, total de filas y columnas de nuestros datos: (filas, columnas)


"""

df.shape

"""Con el método columns podemos ver los nombres de todas las columnas o atributos de nuestros datos:


"""

df.columns

"""Con el método dtypes podemos conocer los tipos de datos de nuestro DataFrame:


"""

df.dtypes

"""Los principales tipos de datos son:

object: cadena de texto
int64 : número entero
float64: número con decimal
datetime64: fecha y hora

La función info( ) nos regresa la información completa de los datos del dataframe:
"""

df.info()

"""Realizaremos una breve exploración de nuestros datos buscando algunas métricas de interés.
Usamos la función sum( ) en la columna Costo, para conocer el total del monto gastado:
"""

df["Costo"].sum()

"""Calculamos el total de tiempo invertido con la función sum( ):


"""

df["Tiempo invertido"].sum()

"""Obtenemos la frecuencia de repetición de las actividades con la función value_counts( ):


"""

df["Nombre actividad"].value_counts()

"""Vemos el conteo de actividades con base en su Momento con la función value_counts( ):"""

df["Momento"].value_counts()

"""Recordamos las etiquetas de los Momentos:

1. Mañana
2. Tarde
3. Noche

Obtenemos el conteo de personas involucradas en las actividades con la función value_counts( ):
"""

df["No. de personas"].value_counts()

"""Usamos la propiedad loc y la función idxmax( ) para obtener la información del costo más máximo o gasto más grande realizado:


"""

df.loc[df["Costo"].idxmax()]

"""Con la función groupby( ) y la función sum( ) obtenemos los totales por Tipo de actividad:


"""

df.groupby("Tipo").sum(numeric_only=True)

"""Recordamos las etiquetas de los Tipos:

1. Alimentos/Salud
2. Ahorro/Inversión
3. Ejercicio/Deporte
4. Entretenimiento/Ocio
5. Académico
6. Transporte

Por último, usamos la función describe( ) para obtener la estadística descriptiva y completar la exploración de nuestros datos:
"""

df.describe()

"""
Seleccionando los datos

Utilizamos la función iloc[ ] para seleccionar los datos deseados de acuerdo a su índice en el DataFrame.
Su sintaxis es iloc[<filas>, <columnas>].
Usamos ":" para indicar que queremos todas las filas y 3:9 para indicar que queremos las columnas de la 4 la 8 del DataFrame"""

df = df.iloc[:,3:9]

"""Usamos la función head( ) para comprobar que seleccionamos correctamente los datos:

"""

df.head()

"""Con la función info( ) vemos la información completa de los datos del DataFrame actualizado.Aquí ya solo debemos tener los valores numéricos int64 o float74.


"""

df.info()

"""Limpiando los datos

Antes de continuar vamos a revisar y limpiar un poco nuestros datos utilizando con el siguiente procedimiento.

Validamos si tenemos valores nulos en alguna columna con la función isnull( ) y sum( ), todos deberían dar cero:
"""

df.isnull().sum()

"""Descartamos aquellos valores nulos o vacíos de nuestro DataFrame con la función dropna( ):


"""

df= df.dropna()

"""Buscamos por última vez valores nulos en todo el DataFrame usando isnull( ) y values.any( ).
Obtenemos True o False dependiendo si hay o no:
"""

df.isnull().values.any()

"""Preparando los datos

El siguiente paso es asignar los atributos(columnas) de nuestros datos a las variables del análisis.

Consultamos los nombres de nuestras columnas con la función columns( ) para asignarlos a las variables:
"""

df.columns

"""La variable x contiene los atributos de entrada y la variable y los atributos de salida.


"""

x = df[['Presupuesto', 'Tiempo invertido', 'Tipo', 'Momento', 'No. de personas']].values # variables independientes
y = df['Costo'].values # variable dependiente

"""Ahora dividimos nuestros datos en un conjunto de entrenamiento (80%) y un conjunto de prueba (20%).
Usaremos los datos de entrenamiento para alimentar al algoritmo de aprendizaje automático para que "aprenda", es decir, genere un modelo para predecir la variable de salida. Usaremos los datos de prueba para realizar la validación del aprendizaje.

Importamos la herramienta para dividir los datos de SciKit-Learn:
"""

from sklearn.model_selection import train_test_split

"""Asignamos 20% de los datos a las x y y de prueba, por lo tanto, tendremos el 80% de los datos para entrenamiento de x y y:


"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

"""Desplegamos una variable para validar que se realizó correctamente la asignación:


"""

y_test

"""Fase 4: Modelación de los Datos
De la librería SciKit-Learn importamos la clase LinearRegression y crearemos un objeto de esa clase con la función LinearRegression( ), que será nuestro modelo de regresión lineal:
"""

from sklearn.linear_model import LinearRegression
model_regression = LinearRegression()

"""Ya con el modelo, utilizamos la función fit( ) para "ajustar"  nuestros datos a un modelo de predicción. Esto permite al modelo "estudiar" nuestros datos y "aprender" de ellos:"""

model_regression.fit(x_train, y_train) # aprendizaje automático con base en nuestros datos

"""En este punto, el algorítmo ya ha aprendido cuales son los coeficientes de x óptimos para satisfacer el modelo. Para desplegarlos creamos un DataFrame indicando los nombres de las variables x y coeficientes:"""

x_labels = ['Presupuesto', 'Tiempo invertido', 'Tipo', 'Momento', 'No. de personas']
c_label = ['Coeficientes']

"""Usamos el método coef_ para obtener los valores de los coeficientes del modelo generado y crear el DataFrame:"""

coeff_df = pd.DataFrame(model_regression.coef_, x_labels, c_label)
coeff_df

"""Vemos los coeficientes; su valor indica su significancia o peso en la variable de salida y.

Ahora que hemos creado nuestro modelo y lo hemos entrenado, es hora de empezar a predecir, usamos la función predict( ) para pedecir los valores de y en función de los valores x de nuestro conjunto de prueba:
"""

y_pred = model_regression.predict(x_test) # realiza la predicción con el modelo generado

"""Revisemos los residuales del modelo, es decir, la diferencia entre los valores reales y los valores de predicción. Creamos un DataFrame con las 3 series: Reales, Predicción, Residuales, obtenemos una muestra de 24 datos y los ordenamos de menor a mayor:"""

residuals = pd.DataFrame({'Real': y_test, 'Predicción': y_pred, 'Residual': y_test - y_pred})
# Take a sample no larger than the DataFrame
sample_size = min(len(residuals), 24)
residuals = residuals.sample(n=sample_size) # Use the calculated sample size
residuals = residuals.sort_values(by='Real')
residuals

"""Ahora calcularemos el coeficiente de determinación R2 para comprobar la precisión de nuestro modelo, mientras mayor sea el R2, mejor será el ajuste del modelo a los datos. Nuestro objetivo es un valor lo más cercano a 1 (100%), que nos indica que tanto se ajusta el modelo a nuestros datos, es decir, que porcentaje de y es explicado por el modelo obtenido.

Importamos la métrica R cuadrada (coeficiente de determinación) de Scikit-Learn y usamos la función r2_score( ) con nuestros valores y:
"""

from sklearn.metrics import r2_score
r2_score(y_test, y_pred)

"""En este caso el valor de R2 es de 0.94 por lo que podemos concluir que el modelo explica solo el 94% del costo de las actividades, lo cual se podrá generalizar, es decir, utilizar para predecir costos. Normalmente se aceptan modelos con R2 mayor 0.68, si tu R2 es menor, tu modelo no será confiable y no se podrán generalizar sus predicciones.

¡En este punto ya creaste con éxito un modelo de regresión lineal multiple, robusto y funcional!

Continua con Visualización de los datos para concluir el análisis.

Visualización de los datos
Por último, hacemos la visualización de los datos mediante un gráfico que nos permita hacer la comparación de los valores reales y de predicción y determinar visualmente que tan preciso o no es nuestro modelo.

Para esto, importamos las librerías matplotlib y numpy, usamos la función .scatter( ) para crear el gráfico de dispersión de puntos:
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np

# %matplotlib inline

# Get the length of the 'Real' column for proper x-axis scaling
num_points = len(residuals['Real'])

plt.scatter(np.arange(num_points), residuals['Real'], label = "Real")
plt.scatter(np.arange(num_points), residuals['Predicción'], label = "Predicción")

plt.title("Comparación de costos: Reales y Predicción")

plt.xlabel("Observaciones de costos")

plt.ylabel("Costos")

plt.legend(loc='upper left')

plt.show()

"""La distancia entre el punto azul y el punto naranja de cada observación representa el error del modelo, menos distancia nos indica que la predicción es precisa, mayor distancia, el error es más grande. En este caso se observa que para casi todos los puntos la distancia es muy pequeña, por lo tanto, el modelo es confiable."""